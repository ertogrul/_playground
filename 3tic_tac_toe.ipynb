{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IijRaWhbrxI"
   },
   "source": [
    "# IBM Think 2020 Code Cafe - Tic Tac Toe with DeepMind OpenSpiel\n",
    "\n",
    "![https://nextgrid.ai/wp-content/uploads/2020/05/ibmthink.png](https://nextgrid.ai/wp-content/uploads/2020/05/ibmthink.png)\n",
    "\n",
    "# What is Reinforcement Learning  (RL)?\n",
    "\n",
    "According  to Wikipedia:\n",
    "> Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward.\n",
    "\n",
    "**In other words: the goal of RL is to learn some program how to behave to get as much reward as possible.**\n",
    "\n",
    "\n",
    "## Basic vocabulary\n",
    "\n",
    "Take a look at the following picture from the *Reinforcement Learning: An Introduction* book by Richard S. Sutton and Andrew Barto:  \n",
    "![alt text](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
    "\n",
    "An **agent** takes an **action** based on the current **state** and gets a **reward** from the **environment ** (please note that reward can be delayed and in timestep *t* agent can receive some reward from an action taken 20 timesteps ago).\n",
    "Agent takes actions until it reaches the terminal state - which indicates the end of the **episode**.\n",
    "\n",
    "\n",
    "## Tic-Tac-Toe example\n",
    "![alt text](https://images.squarespace-cdn.com/content/v1/54f74f23e4b0952b4e0011c0/1580269334204-W1N8ATYATHA6XP02YVSY/ke17ZwdGBToddI8pDm48kGtxPgPaOBG5VTwzK0O3JPx7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmYfwwyaF2qdqpAEW-vwkS-q9yrvcVcBFNcMZ7RZJD-G-L7L3_iLqMJNwF1D5UY19g/tictac.png?format=400w)\n",
    "\n",
    "In a simple game Tic-Tac-Toe, two players play against each other.  \n",
    "One agent puts `X` or `O` in a certain board slot (takes an action) based on the current board state. Then the other agent (environment from the perspective of the first agent) takes its action based on the new board state. And so on.  \n",
    "The game finishes when one agent has 3 `X` or `O` in a row.\n",
    "\n",
    "\n",
    "# Q-Value\n",
    "One of the simplest and the most powerful concepts in RL is **Q-value**.  \n",
    "It is an estimation of **\"how good it to take certain action in a given situation\"**.  \n",
    "For example, you can think of it like of wondering is it good to be Bill Gates and spent money on the charity or deciding if you should go to the safe boat on the Titanic.  \n",
    "\n",
    "In the state of the art RL algorithms, we train a Q-Value function which approximates how good it is to be in a given state and take certain action - it is the fundament of the Deep Q-Network (DQN) - an algorithm which beats human-level performance in the Atari games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqXQjUpNCqLQ"
   },
   "source": [
    "# Import all neccessary libraries.\n",
    "Let's prepare the environment for our playthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdvmS5zrAtae"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jupyter/open_spiel')\n",
    "sys.path.append('/home/jupyter/open_spiel/build/python')  # for pyspiel.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ss85qWHe6qEZ"
   },
   "outputs": [],
   "source": [
    "# verify that Python can find the open_spiel & pyspiel modules\n",
    "import importlib\n",
    "assert importlib.util.find_spec(\"open_spiel\") is not None\n",
    "assert importlib.util.find_spec(\"pyspiel\") is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jh5P8drzpNE7"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import dqn\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "from open_spiel.python.algorithms import tabular_qlearner\n",
    "from open_spiel.python.bots import human\n",
    "\n",
    "game = \"tic_tac_toe\"\n",
    "num_players = 2\n",
    "env = rl_environment.Environment(game)\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "\n",
    "def pretty_board(time_step):\n",
    "    \"\"\"Returns the board in `time_step` in a human readable format.\"\"\"\n",
    "    info_state = time_step.observations[\"info_state\"][0]\n",
    "    x_locations = np.nonzero(info_state[9:18])[0]\n",
    "    o_locations = np.nonzero(info_state[18:])[0]\n",
    "    board = np.full(3 * 3, \".\")\n",
    "    board[x_locations] = \"X\"\n",
    "    board[o_locations] = \"0\"\n",
    "    board = np.reshape(board, (3, 3))\n",
    "    return board\n",
    "\n",
    "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n",
    "    \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "    num_players = len(trained_agents)\n",
    "    sum_episode_rewards = np.zeros(num_players)\n",
    "    for player_pos in range(num_players):\n",
    "        cur_agents = random_agents[:]\n",
    "        cur_agents[player_pos] = trained_agents[player_pos]\n",
    "        for _ in range(num_episodes):\n",
    "            time_step = env.reset()\n",
    "            episode_rewards = 0\n",
    "            while not time_step.last():\n",
    "                player_id = time_step.observations[\"current_player\"]\n",
    "                agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "                action_list = [agent_output.action]\n",
    "                time_step = env.step(action_list)\n",
    "                episode_rewards += time_step.rewards[player_pos]\n",
    "            sum_episode_rewards[player_pos] += episode_rewards\n",
    "    return sum_episode_rewards / num_episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6NKknl_LYdN"
   },
   "source": [
    "# Define two agents\n",
    "\n",
    "It is a simple problem with less than 9^3 different states so we can calculate Q-Value directly and we don't need to use it approximation from DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHY6tTAZL2_J"
   },
   "outputs": [],
   "source": [
    "tabular_q_agent_0 = tabular_qlearner.QLearner(\n",
    "    player_id=0, num_actions=num_actions)\n",
    "tabular_q_agent_1 = tabular_qlearner.QLearner(\n",
    "    player_id=1, num_actions=num_actions)\n",
    "\n",
    "agents = [tabular_q_agent_0, tabular_q_agent_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2JXBGOaMU-U"
   },
   "source": [
    "# Train agents for 100 episodes\n",
    "This agent should be imperfect but better than random one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "bEMQ9GLrMZU_",
    "outputId": "bd02f04a-a895-45ec-c491-6bb33ebecdac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is over.\n"
     ]
    }
   ],
   "source": [
    "train_episodes = 100\n",
    "\n",
    "# Train agent\n",
    "for ep in range(train_episodes):\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        agent_output = agents[player_id].step(time_step)\n",
    "        action_list = [agent_output.action]\n",
    "        time_step = env.step(action_list)\n",
    "\n",
    "    # Episode is over, step all agents with final info state.\n",
    "    for agent in agents:\n",
    "        agent.step(time_step)\n",
    "print(\"Training is over.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mbwS-a2Mid2"
   },
   "source": [
    "# Evaluate first agent playing agains random agent for 1000 episodes\n",
    "Reward -1 means loss  \n",
    "Reward 0 means draw  \n",
    "Reward 1 mean win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "59HaHzbgMq29",
    "outputId": "ac377e2e-b71c-4e95-fb88-3ef01e6ccc87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode rewards: 0.327\n"
     ]
    }
   ],
   "source": [
    "# Evaluate against random agent\n",
    "random_agents = [\n",
    "    random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n",
    "print(\"Mean episode rewards: %s\" % r_mean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ASfRUnPDqIZ"
   },
   "source": [
    "# Lets play with an agent after 100 games of experience\n",
    "You can play against such weak agent. It should be easy to win with it. \n",
    "\n",
    "#### Important\n",
    "You press Enter in input field to see avalible moves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "colab_type": "code",
    "id": "slIsIhCOL2iB",
    "outputId": "99d79ee4-855c-47af-9808-f92fecc69db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are playing as X (you are second)\n",
      "\n",
      "[['.' '.' '.']\n",
      " ['.' '.' '.']\n",
      " ['.' '.' '.']]\n",
      "\n",
      "[['.' '.' '.']\n",
      " ['.' '.' '.']\n",
      " ['.' '0' '.']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  X\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse the action: X\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  3X\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse the action: 3X\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal actions(s):\n",
      "  0: o(0,0)    2: o(0,2)    4: o(1,1)    6: o(2,0)  \n",
      "  1: o(0,1)    3: o(1,0)    5: o(1,2)    8: o(2,2)  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[['X' '.' '.']\n",
      " ['.' '.' '.']\n",
      " ['.' '0' '.']]\n",
      "\n",
      "[['X' '.' '.']\n",
      " ['.' '.' '0']\n",
      " ['.' '0' '.']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal actions(s):\n",
      "  1: o(0,1)    2: o(0,2)    3: o(1,0)    4: o(1,1)    6: o(2,0)    8: o(2,2)  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[['X' '.' '.']\n",
      " ['X' '.' '0']\n",
      " ['.' '0' '.']]\n",
      "\n",
      "[['X' '.' '.']\n",
      " ['X' '.' '0']\n",
      " ['0' '0' '.']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal actions(s):\n",
      "  1: o(0,1)    2: o(0,2)    4: o(1,1)    8: o(2,2)  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[['X' '.' '.']\n",
      " ['X' '.' '0']\n",
      " ['0' '0' 'X']]\n",
      "\n",
      "[['X' '.' '0']\n",
      " ['X' '.' '0']\n",
      " ['0' '0' 'X']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal actions(s):\n",
      "  1: o(0,1)    4: o(1,1)  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of game!\n",
      "You win\n"
     ]
    }
   ],
   "source": [
    "human_player = 1\n",
    "human_agent = human.HumanBot()\n",
    "\n",
    "print(\"You are playing as X (you are second)\" ) \n",
    "time_step = env.reset()\n",
    "while not time_step.last():\n",
    "    player_id = time_step.observations[\"current_player\"]\n",
    "    print(\"\\n%s\" % pretty_board(time_step))\n",
    "\n",
    "    if player_id == human_player:\n",
    "        agent_out = agents[human_player].step(time_step, is_evaluation=True)\n",
    "        action = human_agent.step(env._state)\n",
    "    else:\n",
    "        agent_out = agents[1 - human_player].step(\n",
    "            time_step, is_evaluation=True)\n",
    "        action = agent_out.action\n",
    "    time_step = env.step([action])\n",
    "\n",
    "print(\"End of game!\")\n",
    "if time_step.rewards[human_player] > 0:\n",
    "    print(\"You win\")\n",
    "elif time_step.rewards[human_player] < 0:\n",
    "    print(\"You lose\")\n",
    "else:\n",
    "    print(\"Draw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrpKCxHcNlfU"
   },
   "source": [
    "# Train agents for the 10000 next episodes\n",
    "This will make agent much stronger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "XYakzg6gNlfW",
    "outputId": "a127d426-dfb3-4ea7-a6e4-60751868d390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is over.\n"
     ]
    }
   ],
   "source": [
    "train_episodes = 10000\n",
    "\n",
    "# Train agent\n",
    "for ep in range(train_episodes):\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        agent_output = agents[player_id].step(time_step)\n",
    "        action_list = [agent_output.action]\n",
    "        time_step = env.step(action_list)\n",
    "\n",
    "    # Episode is over, step all agents with final info state.\n",
    "    for agent in agents:\n",
    "        agent.step(time_step)\n",
    "print(\"Training is over.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwylW3rtNlfb"
   },
   "source": [
    "# Evaluate first agent playing agains random agent for 1000 episodes\n",
    "Reward -1 means loss  \n",
    "Reward 0 means draw  \n",
    "Reward 1 mean win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "SCn2-HfjNlfc",
    "outputId": "72e47fdd-7058-4007-df8f-b265bca2e69c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode rewards: 0.941\n"
     ]
    }
   ],
   "source": [
    "# Evaluate against random agent\n",
    "random_agents = [\n",
    "    random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n",
    "print(\"Mean episode rewards: %s\" % r_mean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohnOveFkNlfe"
   },
   "source": [
    "# Now you can play against trained model\n",
    "There is a high probability that you will draw with the current agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "colab_type": "code",
    "id": "8WD1j6EFNlff",
    "outputId": "fa5dec1d-0fa5-447e-97da-744ffc46d680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are playing as X (you are second)\n",
      "\n",
      "[['.' '.' '.']\n",
      " ['.' '.' '.']\n",
      " ['.' '.' '.']]\n",
      "\n",
      "[['.' '0' '.']\n",
      " ['.' '.' '.']\n",
      " ['.' '.' '.']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal actions(s):\n",
      "  0: o(0,0)    3: o(1,0)    5: o(1,2)    7: o(2,1)  \n",
      "  2: o(0,2)    4: o(1,1)    6: o(2,0)    8: o(2,2)  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[['.' '0' '.']\n",
      " ['.' 'X' '.']\n",
      " ['.' '.' '.']]\n",
      "\n",
      "[['.' '0' '.']\n",
      " ['0' 'X' '.']\n",
      " ['.' '.' '.']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal actions(s):\n",
      "  0: o(0,0)    2: o(0,2)    5: o(1,2)    6: o(2,0)    7: o(2,1)    8: o(2,2)  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[['X' '0' '.']\n",
      " ['0' 'X' '.']\n",
      " ['.' '.' '.']]\n",
      "\n",
      "[['X' '0' '.']\n",
      " ['0' 'X' '.']\n",
      " ['.' '.' '0']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal actions(s):\n",
      "  2: o(0,2)    5: o(1,2)    6: o(2,0)    7: o(2,1)  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illegal action selected: 76\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[['X' '0' '.']\n",
      " ['0' 'X' '.']\n",
      " ['X' '.' '0']]\n",
      "\n",
      "[['X' '0' '0']\n",
      " ['0' 'X' '.']\n",
      " ['X' '.' '0']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal actions(s):\n",
      "  5: o(1,2)    7: o(2,1)  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an action (empty to print legal actions):  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[['X' '0' '0']\n",
      " ['0' 'X' 'X']\n",
      " ['X' '.' '0']]\n",
      "End of game!\n",
      "Draw\n"
     ]
    }
   ],
   "source": [
    "human_player = 1\n",
    "human_agent = human.HumanBot()\n",
    "\n",
    "print(\"You are playing as X (you are second)\" ) \n",
    "time_step = env.reset()\n",
    "while not time_step.last():\n",
    "    player_id = time_step.observations[\"current_player\"]\n",
    "    print(\"\\n%s\" % pretty_board(time_step))\n",
    "\n",
    "    if player_id == human_player:\n",
    "        agent_out = agents[human_player].step(time_step, is_evaluation=True)\n",
    "        action = human_agent.step(env._state)\n",
    "    else:\n",
    "        agent_out = agents[1 - human_player].step(\n",
    "            time_step, is_evaluation=True)\n",
    "        action = agent_out.action\n",
    "    time_step = env.step([action])\n",
    "\n",
    "print(\"End of game!\")\n",
    "if time_step.rewards[human_player] > 0:\n",
    "    print(\"You win\")\n",
    "elif time_step.rewards[human_player] < 0:\n",
    "    print(\"You lose\")\n",
    "else:\n",
    "    print(\"Draw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELNSVl3sOTbu"
   },
   "source": [
    "# Let visualize Q-value for some states\n",
    "Here you can see how certain actions are evaluated by the agent.  \n",
    "It should take action with the highest value.\n",
    "\n",
    "You can run this cell multiple times to see different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zC0dtHeRD4W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled state:\n",
      "[['   O   ' '   .   ' '   .   ']\n",
      " ['   X   ' '   X   ' '   O   ']\n",
      " ['   .   ' '   X   ' '   O   ']]\n",
      "\n",
      "Q-Values:\n",
      "[['   O   ' '   0.01' '    0.0']\n",
      " ['   X   ' '   X   ' '   O   ']\n",
      " ['    0.0' '   X   ' '   O   ']]\n"
     ]
    }
   ],
   "source": [
    "def pretty_decoded_board(state_decoded, q_values):\n",
    "    \"\"\"Returns the board in `time_step` in a human readable format.\"\"\"\n",
    "    info_state = state_decoded\n",
    "    x_locations = np.nonzero(info_state[9:18])[0]\n",
    "    o_locations = np.nonzero(info_state[18:])[0]\n",
    "    board = np.full(3 * 3, \"   .   \")\n",
    "    board[x_locations] = \"   X   \"\n",
    "    board[o_locations] = \"   O   \"\n",
    "    for index, q_val in q_values.items():\n",
    "        board[index] = f\"{q_val:7.5}\"\n",
    "\n",
    "    board = np.reshape(board, (3, 3))\n",
    "    return board\n",
    "    \n",
    "agent = agents[1 - human_player]\n",
    "\n",
    "q_values_are_zeros = True\n",
    "while q_values_are_zeros:\n",
    "    state_hash = np.random.choice(list(agent._q_values))\n",
    "    state_decoded = [float(i) for i in state_hash[1: -1].split(\",\")]  # 27 variables (9 * len([O, X, empty]))\n",
    "    q_values = agent._q_values[state_hash]\n",
    "    q_values_are_zeros = (np.array(list(q_values.values())) == 0).all()\n",
    "\n",
    "print(\"Sampled state:\")\n",
    "print(pretty_decoded_board(state_decoded, {}))\n",
    "\n",
    "print(\"\\nQ-Values:\")\n",
    "print(pretty_decoded_board(state_decoded, q_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OpenSpiel@IBM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "virtualenv-3.7",
   "language": "python",
   "name": "virtualenv-3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
